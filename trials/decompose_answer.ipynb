{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given input data\n",
    "data = {\n",
    "    'question': 'What is a large language model. Use 5 sentences, to describe it?',\n",
    "    'chat_history': [],\n",
    "    'answer': ' A large language model is a type of artificial neural network that is designed to process and interpret language. It is trained on a large dataset of words and sequences of words, which allows it to learn the patterns and structures of human language. It is capable of generating text that is similar to that of a human, and can be used for tasks such as machine translation or text and speech generation. Large language models are often based on feedforward networks, which are a type of artificial neural network. They are organized into different layers, with the input layer receiving raw input data from the user and the output layer producing a prediction.',\n",
    "    'source_documents': [\n",
    "        {\n",
    "            'page_content': 'Text-in-Bild oder Text-in-Audio Transformationen eingesetzt werden, wie etwa die Anwendung Dall-\\n\\nE zeigt. Large Language Models sind daher nicht nur in ihrem (vermeintlich) primären \\n\\nAnwendungsbereich, der Sprach- und Textgenerierung, sondern auch vor dem Hintergrund \\n\\nsekundärer Anwendungsbereiche, zu denken.\\n\\nII. Problemkreise von Large Language Models und deren Erzeugnissen\\n\\nBezüglich der Verwendung von Large Language Models können mehrere Problemkreise, die hier nur \\n\\nüberblicksartig dargestellt werden können, identifiziert werden (umfassend betrachtend Weidinger \\n\\net al., Ethical and social risks of harm from Language Models, 2021). Ein zentrales Problem liegt \\n\\nbereits darin, dass – trotz häufig überwiegend guter Text- oder Sprachausgaben – der generierte \\n\\nText unsinnig sein oder grammatikalische Fehler enthalten kann. Dies kann passieren, wenn das \\n\\nSprachmodell in den Trainingsdaten Muster gelernt hat, die im realen Sprachgebrauch nicht',\n",
    "            'metadata': {'source': '/Users/swmoeller/python/2023/large_language_model/BrainGPT/data/10_raw/doc2scan/Regulierung_von_Large_Language_Models_in_DSA_und_AIA-E_-_beck-online.pdf'}\n",
    "        },\n",
    "        {\n",
    "            'page_content': 'Redaktion MMR-Aktuell\\n\\nKurzbeiträge/Kommentare\\n\\nMMR-Aktuell 2023, 455171\\n\\nRegulierung von Large Language Models in DSA und AIA-E\\n\\nAlexander Wehde ist studentische Hilfskraft am Lehrstuhl für Bürgerliches Recht, Informations- und \\n\\nDatenrecht bei Prof. Dr. Louisa Specht-Riemenschneider an der Rheinischen-Friedrich-Wilhelms-\\n\\nUniversität Bonn und stellv. Vorstandsvorsitzender der Forschungsstelle für Rechtsfragen neuer \\n\\nTechnologien sowie Datenrecht (ForTech) e.V.\\n\\nDie Regulierung großer Sprachmodelle (engl. Large Language Models) ist diffizil, befindet sie sich \\n\\ndoch im Spannungsfeld von Innovationsförderung und mannigfaltigen Befürchtungen über die \\n\\ngenaue Verwendung sowie Ausgestaltung solcher Systeme. Letztere erwachsen dabei insbesondere \\n\\naufgrund der Tatsache, dass Large Language Models in der Lage sind, Text zu erzeugen, der dem \\n\\nLeser den Eindruck vermitteln mag von Menschen geschriebener Text läge vor (sog. ELIZA-Effekt).',\n",
    "            'metadata': {'source': '/Users/swmoeller/python/2023/large_language_model/BrainGPT/data/10_raw/doc2scan/Regulierung_von_Large_Language_Models_in_DSA_und_AIA-E_-_beck-online.pdf'}\n",
    "        },\n",
    "        {\n",
    "            'page_content': 'für die Verarbeitung von Sprach- oder Texteingaben verwendet, wie zB im Rahmen maschineller \\n\\nÜbersetzungen oder zur Sprach- und Textgenerierung.\\n\\nTechnisch arbeiten Large Language Models häufig auf Basis sog. Feedforward Networks, die als \\n\\nUnterform künstlicher neuronaler Netzwerke beschrieben werden können (zur Architekur von Large \\n\\nLanguage Models etwa Vaswani et al., Attention Is All You Need, NIPS 2017). Als solche orientieren \\n\\nsich künstliche neuronale Netzwerke grundsätzlich am Erkenntnisstand der Arbeitsweise des \\n\\nmenschlichen Gehirns und bestehen aus miteinander verbundenen künstlichen Neuronen, die Daten \\n\\nverarbeiten und verknüpfen können. Im Kontext von Large Language Models sind diese im \\n\\nkünstlichen neuronalen Netzwerk in unterschiedlichen bzw. mehreren „Layern“ organisiert, wobei \\n\\nder Input Layer die rohen Eingabedaten des Nutzenden erhält und der Output Layer die \\n\\nAusgabevorhersage erstellt. Zwischen dem Input- und Output Layer liegen mehrere sog. Hidden',\n",
    "            'metadata': {'source': '/Users/swmoeller/python/2023/large_language_model/BrainGPT/data/10_raw/doc2scan/Regulierung_von_Large_Language_Models_in_DSA_und_AIA-E_-_beck-online.pdf'}\n",
    "        },\n",
    "        {\n",
    "            'page_content': 'Vor diesem Hintergrund stellt sich die Frage, inwieweit die angestrebte bzw. bereits verankerte \\n\\ndigitalpolitische Regulierung in Gestalt des Digital Services Act (DSA) und des Komissionsentwurfs \\n\\nzum Artificial Intelligence Act (AIA-E) Large Language Models angemessen berücksichtigen, um \\n\\neinen ethischen und verantwortungsbewussten Einsatz dieser in Zukunft sicherzustellen.\\n\\nI. Large Language Models\\n\\nLarge Language Models wie GPT-3, BERT oder Blender Bot 3 erfreuen sich dieser Tage großer \\n\\nAufmerksamkeit. Die Bezeichnung dieser als „large“ stellt dabei einen Rückbezug auf den sehr \\n\\ngroßen Datensatz anhand dessen das System trainiert wurde dar. Jener besteht oftmals aus \\n\\nMilliarden von Wörtern und Abfolgen dieser, was es den Large Language Models ermöglicht die \\n\\nMuster und Strukturen menschlicher Sprache verblüffend präzise zu erlernen und Text zu \\n\\ngenerieren, der dem eines Menschen ähnlich ist. Heute werden große Sprachmodelle daher häufig',\n",
    "            'metadata': {'source': '/Users/swmoeller/python/2023/large_language_model/BrainGPT/data/10_raw/doc2scan/Regulierung_von_Large_Language_Models_in_DSA_und_AIA-E_-_beck-online.pdf'}\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Extract the question, answer, page content, and source\n",
    "question = data['question']\n",
    "answer = data['answer']\n",
    "page_contents = [doc['page_content'] for doc in data['source_documents']]\n",
    "sources = [doc['metadata']['source'] for doc in data['source_documents']]\n",
    "\n",
    "# Print the extracted information\n",
    "print(\"Question:\", question)\n",
    "print(\"\\nAnswer:\", answer)\n",
    "print(\"\\nPage Content:\")\n",
    "for i, content in enumerate(page_contents):\n",
    "    print(f\"Page {i+1}:\")\n",
    "    print(content)\n",
    "    print()\n",
    "print(\"\\nSources:\")\n",
    "for source in sources:\n",
    "    print(source)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Document' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/swmoeller/python/2023/large_language_model/BrainGPT/scripts/trials/decompose_answer.ipynb Cell 2\u001b[0m line \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/swmoeller/python/2023/large_language_model/BrainGPT/scripts/trials/decompose_answer.ipynb#W1sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mjson\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/swmoeller/python/2023/large_language_model/BrainGPT/scripts/trials/decompose_answer.ipynb#W1sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mre\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/swmoeller/python/2023/large_language_model/BrainGPT/scripts/trials/decompose_answer.ipynb#W1sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m data \u001b[39m=\u001b[39m{\u001b[39m'\u001b[39m\u001b[39mquestion\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39mWhat is a large language model. Use 5 sentences, to describe it?\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mchat_history\u001b[39m\u001b[39m'\u001b[39m: [], \u001b[39m'\u001b[39m\u001b[39manswer\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39m A large language model is a type of artificial neural network that is designed to process and interpret language. It is trained on a large dataset of words and sequences of words, which allows it to learn the patterns and structures of human language. It is capable of generating text that is similar to that of a human, and can be used for tasks such as machine translation or text and speech generation. Large language models are often based on feedforward networks, which are a type of artificial neural network. They are organized into different layers, with the input layer receiving raw input data from the user and the output layer producing a prediction.\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39msource_documents\u001b[39m\u001b[39m'\u001b[39m: [Document(page_content\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mText-in-Bild oder Text-in-Audio Transformationen eingesetzt werden, wie etwa die Anwendung Dall-\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39mE zeigt. Large Language Models sind daher nicht nur in ihrem (vermeintlich) primären \u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39mAnwendungsbereich, der Sprach- und Textgenerierung, sondern auch vor dem Hintergrund \u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39msekundärer Anwendungsbereiche, zu denken.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39mII. Problemkreise von Large Language Models und deren Erzeugnissen\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39mBezüglich der Verwendung von Large Language Models können mehrere Problemkreise, die hier nur \u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39müberblicksartig dargestellt werden können, identifiziert werden (umfassend betrachtend Weidinger \u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39met al., Ethical and social risks of harm from Language Models, 2021). Ein zentrales Problem liegt \u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39mbereits darin, dass – trotz häufig überwiegend guter Text- oder Sprachausgaben – der generierte \u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39mText unsinnig sein oder grammatikalische Fehler enthalten kann. Dies kann passieren, wenn das \u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39mSprachmodell in den Trainingsdaten Muster gelernt hat, die im realen Sprachgebrauch nicht\u001b[39m\u001b[39m'\u001b[39m, metadata\u001b[39m=\u001b[39m{\u001b[39m'\u001b[39m\u001b[39msource\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39m/Users/swmoeller/python/2023/large_language_model/BrainGPT/data/10_raw/doc2scan/Regulierung_von_Large_Language_Models_in_DSA_und_AIA-E_-_beck-online.pdf\u001b[39m\u001b[39m'\u001b[39m}), Document(page_content\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mRedaktion MMR-Aktuell\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39mKurzbeiträge/Kommentare\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39mMMR-Aktuell 2023, 455171\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39mRegulierung von Large Language Models in DSA und AIA-E\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39mAlexander Wehde ist studentische Hilfskraft am Lehrstuhl für Bürgerliches Recht, Informations- und \u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39mDatenrecht bei Prof. Dr. Louisa Specht-Riemenschneider an der Rheinischen-Friedrich-Wilhelms-\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39mUniversität Bonn und stellv. Vorstandsvorsitzender der Forschungsstelle für Rechtsfragen neuer \u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39mTechnologien sowie Datenrecht (ForTech) e.V.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39mDie Regulierung großer Sprachmodelle (engl. Large Language Models) ist diffizil, befindet sie sich \u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39mdoch im Spannungsfeld von Innovationsförderung und mannigfaltigen Befürchtungen über die \u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39mgenaue Verwendung sowie Ausgestaltung solcher Systeme. Letztere erwachsen dabei insbesondere \u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39maufgrund der Tatsache, dass Large Language Models in der Lage sind, Text zu erzeugen, der dem \u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39mLeser den Eindruck vermitteln mag von Menschen geschriebener Text läge vor (sog. ELIZA-Effekt).\u001b[39m\u001b[39m'\u001b[39m, metadata\u001b[39m=\u001b[39m{\u001b[39m'\u001b[39m\u001b[39msource\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39m/Users/swmoeller/python/2023/large_language_model/BrainGPT/data/10_raw/doc2scan/Regulierung_von_Large_Language_Models_in_DSA_und_AIA-E_-_beck-online.pdf\u001b[39m\u001b[39m'\u001b[39m}), Document(page_content\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mfür die Verarbeitung von Sprach- oder Texteingaben verwendet, wie zB im Rahmen maschineller \u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39mÜbersetzungen oder zur Sprach- und Textgenerierung.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39mTechnisch arbeiten Large Language Models häufig auf Basis sog. Feedforward Networks, die als \u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39mUnterform künstlicher neuronaler Netzwerke beschrieben werden können (zur Architekur von Large \u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39mLanguage Models etwa Vaswani et al., Attention Is All You Need, NIPS 2017). Als solche orientieren \u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39msich künstliche neuronale Netzwerke grundsätzlich am Erkenntnisstand der Arbeitsweise des \u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39mmenschlichen Gehirns und bestehen aus miteinander verbundenen künstlichen Neuronen, die Daten \u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39mverarbeiten und verknüpfen können. Im Kontext von Large Language Models sind diese im \u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39mkünstlichen neuronalen Netzwerk in unterschiedlichen bzw. mehreren „Layern“ organisiert, wobei \u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39mder Input Layer die rohen Eingabedaten des Nutzenden erhält und der Output Layer die \u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39mAusgabevorhersage erstellt. Zwischen dem Input- und Output Layer liegen mehrere sog. Hidden\u001b[39m\u001b[39m'\u001b[39m, metadata\u001b[39m=\u001b[39m{\u001b[39m'\u001b[39m\u001b[39msource\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39m/Users/swmoeller/python/2023/large_language_model/BrainGPT/data/10_raw/doc2scan/Regulierung_von_Large_Language_Models_in_DSA_und_AIA-E_-_beck-online.pdf\u001b[39m\u001b[39m'\u001b[39m}), Document(page_content\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mVor diesem Hintergrund stellt sich die Frage, inwieweit die angestrebte bzw. bereits verankerte \u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39mdigitalpolitische Regulierung in Gestalt des Digital Services Act (DSA) und des Komissionsentwurfs \u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39mzum Artificial Intelligence Act (AIA-E) Large Language Models angemessen berücksichtigen, um \u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39meinen ethischen und verantwortungsbewussten Einsatz dieser in Zukunft sicherzustellen.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39mI. Large Language Models\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39mLarge Language Models wie GPT-3, BERT oder Blender Bot 3 erfreuen sich dieser Tage großer \u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39mAufmerksamkeit. Die Bezeichnung dieser als „large“ stellt dabei einen Rückbezug auf den sehr \u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39mgroßen Datensatz anhand dessen das System trainiert wurde dar. Jener besteht oftmals aus \u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39mMilliarden von Wörtern und Abfolgen dieser, was es den Large Language Models ermöglicht die \u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39mMuster und Strukturen menschlicher Sprache verblüffend präzise zu erlernen und Text zu \u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39mgenerieren, der dem eines Menschen ähnlich ist. Heute werden große Sprachmodelle daher häufig\u001b[39m\u001b[39m'\u001b[39m, metadata\u001b[39m=\u001b[39m{\u001b[39m'\u001b[39m\u001b[39msource\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39m/Users/swmoeller/python/2023/large_language_model/BrainGPT/data/10_raw/doc2scan/Regulierung_von_Large_Language_Models_in_DSA_und_AIA-E_-_beck-online.pdf\u001b[39m\u001b[39m'\u001b[39m})]}\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/swmoeller/python/2023/large_language_model/BrainGPT/scripts/trials/decompose_answer.ipynb#W1sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39m# Replace single quotes with double quotes using regex\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/swmoeller/python/2023/large_language_model/BrainGPT/scripts/trials/decompose_answer.ipynb#W1sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39m#input_text = re.sub(r\"\\'(.*?)\\'\", r'\"\\1\"', input_text)\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/swmoeller/python/2023/large_language_model/BrainGPT/scripts/trials/decompose_answer.ipynb#W1sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/swmoeller/python/2023/large_language_model/BrainGPT/scripts/trials/decompose_answer.ipynb#W1sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/swmoeller/python/2023/large_language_model/BrainGPT/scripts/trials/decompose_answer.ipynb#W1sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39m# Extract specific elements\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/swmoeller/python/2023/large_language_model/BrainGPT/scripts/trials/decompose_answer.ipynb#W1sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m question \u001b[39m=\u001b[39m data[\u001b[39m'\u001b[39m\u001b[39mquestion\u001b[39m\u001b[39m'\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Document' is not defined"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "data ={'question': 'What is a large language model. Use 5 sentences, to describe it?', 'chat_history': [], 'answer': ' A large language model is a type of artificial neural network that is designed to process and interpret language. It is trained on a large dataset of words and sequences of words, which allows it to learn the patterns and structures of human language. It is capable of generating text that is similar to that of a human, and can be used for tasks such as machine translation or text and speech generation. Large language models are often based on feedforward networks, which are a type of artificial neural network. They are organized into different layers, with the input layer receiving raw input data from the user and the output layer producing a prediction.', 'source_documents': [Document(page_content='Text-in-Bild oder Text-in-Audio Transformationen eingesetzt werden, wie etwa die Anwendung Dall-\\n\\nE zeigt. Large Language Models sind daher nicht nur in ihrem (vermeintlich) primären \\n\\nAnwendungsbereich, der Sprach- und Textgenerierung, sondern auch vor dem Hintergrund \\n\\nsekundärer Anwendungsbereiche, zu denken.\\n\\nII. Problemkreise von Large Language Models und deren Erzeugnissen\\n\\nBezüglich der Verwendung von Large Language Models können mehrere Problemkreise, die hier nur \\n\\nüberblicksartig dargestellt werden können, identifiziert werden (umfassend betrachtend Weidinger \\n\\net al., Ethical and social risks of harm from Language Models, 2021). Ein zentrales Problem liegt \\n\\nbereits darin, dass – trotz häufig überwiegend guter Text- oder Sprachausgaben – der generierte \\n\\nText unsinnig sein oder grammatikalische Fehler enthalten kann. Dies kann passieren, wenn das \\n\\nSprachmodell in den Trainingsdaten Muster gelernt hat, die im realen Sprachgebrauch nicht', metadata={'source': '/Users/swmoeller/python/2023/large_language_model/BrainGPT/data/10_raw/doc2scan/Regulierung_von_Large_Language_Models_in_DSA_und_AIA-E_-_beck-online.pdf'}), Document(page_content='Redaktion MMR-Aktuell\\n\\nKurzbeiträge/Kommentare\\n\\nMMR-Aktuell 2023, 455171\\n\\nRegulierung von Large Language Models in DSA und AIA-E\\n\\nAlexander Wehde ist studentische Hilfskraft am Lehrstuhl für Bürgerliches Recht, Informations- und \\n\\nDatenrecht bei Prof. Dr. Louisa Specht-Riemenschneider an der Rheinischen-Friedrich-Wilhelms-\\n\\nUniversität Bonn und stellv. Vorstandsvorsitzender der Forschungsstelle für Rechtsfragen neuer \\n\\nTechnologien sowie Datenrecht (ForTech) e.V.\\n\\nDie Regulierung großer Sprachmodelle (engl. Large Language Models) ist diffizil, befindet sie sich \\n\\ndoch im Spannungsfeld von Innovationsförderung und mannigfaltigen Befürchtungen über die \\n\\ngenaue Verwendung sowie Ausgestaltung solcher Systeme. Letztere erwachsen dabei insbesondere \\n\\naufgrund der Tatsache, dass Large Language Models in der Lage sind, Text zu erzeugen, der dem \\n\\nLeser den Eindruck vermitteln mag von Menschen geschriebener Text läge vor (sog. ELIZA-Effekt).', metadata={'source': '/Users/swmoeller/python/2023/large_language_model/BrainGPT/data/10_raw/doc2scan/Regulierung_von_Large_Language_Models_in_DSA_und_AIA-E_-_beck-online.pdf'}), Document(page_content='für die Verarbeitung von Sprach- oder Texteingaben verwendet, wie zB im Rahmen maschineller \\n\\nÜbersetzungen oder zur Sprach- und Textgenerierung.\\n\\nTechnisch arbeiten Large Language Models häufig auf Basis sog. Feedforward Networks, die als \\n\\nUnterform künstlicher neuronaler Netzwerke beschrieben werden können (zur Architekur von Large \\n\\nLanguage Models etwa Vaswani et al., Attention Is All You Need, NIPS 2017). Als solche orientieren \\n\\nsich künstliche neuronale Netzwerke grundsätzlich am Erkenntnisstand der Arbeitsweise des \\n\\nmenschlichen Gehirns und bestehen aus miteinander verbundenen künstlichen Neuronen, die Daten \\n\\nverarbeiten und verknüpfen können. Im Kontext von Large Language Models sind diese im \\n\\nkünstlichen neuronalen Netzwerk in unterschiedlichen bzw. mehreren „Layern“ organisiert, wobei \\n\\nder Input Layer die rohen Eingabedaten des Nutzenden erhält und der Output Layer die \\n\\nAusgabevorhersage erstellt. Zwischen dem Input- und Output Layer liegen mehrere sog. Hidden', metadata={'source': '/Users/swmoeller/python/2023/large_language_model/BrainGPT/data/10_raw/doc2scan/Regulierung_von_Large_Language_Models_in_DSA_und_AIA-E_-_beck-online.pdf'}), Document(page_content='Vor diesem Hintergrund stellt sich die Frage, inwieweit die angestrebte bzw. bereits verankerte \\n\\ndigitalpolitische Regulierung in Gestalt des Digital Services Act (DSA) und des Komissionsentwurfs \\n\\nzum Artificial Intelligence Act (AIA-E) Large Language Models angemessen berücksichtigen, um \\n\\neinen ethischen und verantwortungsbewussten Einsatz dieser in Zukunft sicherzustellen.\\n\\nI. Large Language Models\\n\\nLarge Language Models wie GPT-3, BERT oder Blender Bot 3 erfreuen sich dieser Tage großer \\n\\nAufmerksamkeit. Die Bezeichnung dieser als „large“ stellt dabei einen Rückbezug auf den sehr \\n\\ngroßen Datensatz anhand dessen das System trainiert wurde dar. Jener besteht oftmals aus \\n\\nMilliarden von Wörtern und Abfolgen dieser, was es den Large Language Models ermöglicht die \\n\\nMuster und Strukturen menschlicher Sprache verblüffend präzise zu erlernen und Text zu \\n\\ngenerieren, der dem eines Menschen ähnlich ist. Heute werden große Sprachmodelle daher häufig', metadata={'source': '/Users/swmoeller/python/2023/large_language_model/BrainGPT/data/10_raw/doc2scan/Regulierung_von_Large_Language_Models_in_DSA_und_AIA-E_-_beck-online.pdf'})]}\n",
    "\n",
    "\n",
    "# Replace single quotes with double quotes using regex\n",
    "#input_text = re.sub(r\"\\'(.*?)\\'\", r'\"\\1\"', input_text)\n",
    "\n",
    "# Parse the input as a dictionary\n",
    "#data = json.loads(input_text)\n",
    "\n",
    "# Extract specific elements\n",
    "question = data['question']\n",
    "answer = data['answer']\n",
    "source_documents = data['source_documents'][0]\n",
    "\n",
    "# Print the extracted elements\n",
    "print(\"Question:\", question)\n",
    "print(\"Answer:\", answer)\n",
    "print(\"Source Documents:\", source_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Document' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/swmoeller/python/2023/large_language_model/BrainGPT/scripts/trials/decompose_answer.ipynb Cell 3\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/swmoeller/python/2023/large_language_model/BrainGPT/scripts/trials/decompose_answer.ipynb#W3sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m your_data \u001b[39m=\u001b[39m{\u001b[39m'\u001b[39m\u001b[39mquestion\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39mWhat is a large language model. Use 5 sentences, to describe it?\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mchat_history\u001b[39m\u001b[39m'\u001b[39m: [], \u001b[39m'\u001b[39m\u001b[39manswer\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39m A large language model is a type of artificial neural network that is designed to process and interpret language. It is trained on a large dataset of words and sequences of words, which allows it to learn the patterns and structures of human language. It is capable of generating text that is similar to that of a human, and can be used for tasks such as machine translation or text and speech generation. Large language models are often based on feedforward networks, which are a type of artificial neural network. They are organized into different layers, with the input layer receiving raw input data from the user and the output layer producing a prediction.\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39msource_documents\u001b[39m\u001b[39m'\u001b[39m: [Document(page_content\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mText-in-Bild oder Text-in-Audio Transformationen eingesetzt werden, wie etwa die Anwendung Dall-\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39mE zeigt. Large Language Models sind daher nicht nur in ihrem (vermeintlich) primären \u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39mAnwendungsbereich, der Sprach- und Textgenerierung, sondern auch vor dem Hintergrund \u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39msekundärer Anwendungsbereiche, zu denken.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39mII. Problemkreise von Large Language Models und deren Erzeugnissen\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39mBezüglich der Verwendung von Large Language Models können mehrere Problemkreise, die hier nur \u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39müberblicksartig dargestellt werden können, identifiziert werden (umfassend betrachtend Weidinger \u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39met al., Ethical and social risks of harm from Language Models, 2021). Ein zentrales Problem liegt \u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39mbereits darin, dass – trotz häufig überwiegend guter Text- oder Sprachausgaben – der generierte \u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39mText unsinnig sein oder grammatikalische Fehler enthalten kann. Dies kann passieren, wenn das \u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39mSprachmodell in den Trainingsdaten Muster gelernt hat, die im realen Sprachgebrauch nicht\u001b[39m\u001b[39m'\u001b[39m, metadata\u001b[39m=\u001b[39m{\u001b[39m'\u001b[39m\u001b[39msource\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39m/Users/swmoeller/python/2023/large_language_model/BrainGPT/data/10_raw/doc2scan/Regulierung_von_Large_Language_Models_in_DSA_und_AIA-E_-_beck-online.pdf\u001b[39m\u001b[39m'\u001b[39m}), Document(page_content\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mRedaktion MMR-Aktuell\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39mKurzbeiträge/Kommentare\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39mMMR-Aktuell 2023, 455171\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39mRegulierung von Large Language Models in DSA und AIA-E\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39mAlexander Wehde ist studentische Hilfskraft am Lehrstuhl für Bürgerliches Recht, Informations- und \u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39mDatenrecht bei Prof. Dr. Louisa Specht-Riemenschneider an der Rheinischen-Friedrich-Wilhelms-\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39mUniversität Bonn und stellv. Vorstandsvorsitzender der Forschungsstelle für Rechtsfragen neuer \u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39mTechnologien sowie Datenrecht (ForTech) e.V.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39mDie Regulierung großer Sprachmodelle (engl. Large Language Models) ist diffizil, befindet sie sich \u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39mdoch im Spannungsfeld von Innovationsförderung und mannigfaltigen Befürchtungen über die \u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39mgenaue Verwendung sowie Ausgestaltung solcher Systeme. Letztere erwachsen dabei insbesondere \u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39maufgrund der Tatsache, dass Large Language Models in der Lage sind, Text zu erzeugen, der dem \u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39mLeser den Eindruck vermitteln mag von Menschen geschriebener Text läge vor (sog. ELIZA-Effekt).\u001b[39m\u001b[39m'\u001b[39m, metadata\u001b[39m=\u001b[39m{\u001b[39m'\u001b[39m\u001b[39msource\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39m/Users/swmoeller/python/2023/large_language_model/BrainGPT/data/10_raw/doc2scan/Regulierung_von_Large_Language_Models_in_DSA_und_AIA-E_-_beck-online.pdf\u001b[39m\u001b[39m'\u001b[39m}), Document(page_content\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mfür die Verarbeitung von Sprach- oder Texteingaben verwendet, wie zB im Rahmen maschineller \u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39mÜbersetzungen oder zur Sprach- und Textgenerierung.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39mTechnisch arbeiten Large Language Models häufig auf Basis sog. Feedforward Networks, die als \u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39mUnterform künstlicher neuronaler Netzwerke beschrieben werden können (zur Architekur von Large \u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39mLanguage Models etwa Vaswani et al., Attention Is All You Need, NIPS 2017). Als solche orientieren \u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39msich künstliche neuronale Netzwerke grundsätzlich am Erkenntnisstand der Arbeitsweise des \u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39mmenschlichen Gehirns und bestehen aus miteinander verbundenen künstlichen Neuronen, die Daten \u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39mverarbeiten und verknüpfen können. Im Kontext von Large Language Models sind diese im \u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39mkünstlichen neuronalen Netzwerk in unterschiedlichen bzw. mehreren „Layern“ organisiert, wobei \u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39mder Input Layer die rohen Eingabedaten des Nutzenden erhält und der Output Layer die \u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39mAusgabevorhersage erstellt. Zwischen dem Input- und Output Layer liegen mehrere sog. Hidden\u001b[39m\u001b[39m'\u001b[39m, metadata\u001b[39m=\u001b[39m{\u001b[39m'\u001b[39m\u001b[39msource\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39m/Users/swmoeller/python/2023/large_language_model/BrainGPT/data/10_raw/doc2scan/Regulierung_von_Large_Language_Models_in_DSA_und_AIA-E_-_beck-online.pdf\u001b[39m\u001b[39m'\u001b[39m}), Document(page_content\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mVor diesem Hintergrund stellt sich die Frage, inwieweit die angestrebte bzw. bereits verankerte \u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39mdigitalpolitische Regulierung in Gestalt des Digital Services Act (DSA) und des Komissionsentwurfs \u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39mzum Artificial Intelligence Act (AIA-E) Large Language Models angemessen berücksichtigen, um \u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39meinen ethischen und verantwortungsbewussten Einsatz dieser in Zukunft sicherzustellen.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39mI. Large Language Models\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39mLarge Language Models wie GPT-3, BERT oder Blender Bot 3 erfreuen sich dieser Tage großer \u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39mAufmerksamkeit. Die Bezeichnung dieser als „large“ stellt dabei einen Rückbezug auf den sehr \u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39mgroßen Datensatz anhand dessen das System trainiert wurde dar. Jener besteht oftmals aus \u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39mMilliarden von Wörtern und Abfolgen dieser, was es den Large Language Models ermöglicht die \u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39mMuster und Strukturen menschlicher Sprache verblüffend präzise zu erlernen und Text zu \u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39mgenerieren, der dem eines Menschen ähnlich ist. Heute werden große Sprachmodelle daher häufig\u001b[39m\u001b[39m'\u001b[39m, metadata\u001b[39m=\u001b[39m{\u001b[39m'\u001b[39m\u001b[39msource\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39m/Users/swmoeller/python/2023/large_language_model/BrainGPT/data/10_raw/doc2scan/Regulierung_von_Large_Language_Models_in_DSA_und_AIA-E_-_beck-online.pdf\u001b[39m\u001b[39m'\u001b[39m})]}\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/swmoeller/python/2023/large_language_model/BrainGPT/scripts/trials/decompose_answer.ipynb#W3sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m extracted_data \u001b[39m=\u001b[39m []\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/swmoeller/python/2023/large_language_model/BrainGPT/scripts/trials/decompose_answer.ipynb#W3sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39m#Create list of all items page_content, metadata, score\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Document' is not defined"
     ]
    }
   ],
   "source": [
    "your_data ={'question': 'What is a large language model. Use 5 sentences, to describe it?', 'chat_history': [], 'answer': ' A large language model is a type of artificial neural network that is designed to process and interpret language. It is trained on a large dataset of words and sequences of words, which allows it to learn the patterns and structures of human language. It is capable of generating text that is similar to that of a human, and can be used for tasks such as machine translation or text and speech generation. Large language models are often based on feedforward networks, which are a type of artificial neural network. They are organized into different layers, with the input layer receiving raw input data from the user and the output layer producing a prediction.', 'source_documents': [Document(page_content='Text-in-Bild oder Text-in-Audio Transformationen eingesetzt werden, wie etwa die Anwendung Dall-\\n\\nE zeigt. Large Language Models sind daher nicht nur in ihrem (vermeintlich) primären \\n\\nAnwendungsbereich, der Sprach- und Textgenerierung, sondern auch vor dem Hintergrund \\n\\nsekundärer Anwendungsbereiche, zu denken.\\n\\nII. Problemkreise von Large Language Models und deren Erzeugnissen\\n\\nBezüglich der Verwendung von Large Language Models können mehrere Problemkreise, die hier nur \\n\\nüberblicksartig dargestellt werden können, identifiziert werden (umfassend betrachtend Weidinger \\n\\net al., Ethical and social risks of harm from Language Models, 2021). Ein zentrales Problem liegt \\n\\nbereits darin, dass – trotz häufig überwiegend guter Text- oder Sprachausgaben – der generierte \\n\\nText unsinnig sein oder grammatikalische Fehler enthalten kann. Dies kann passieren, wenn das \\n\\nSprachmodell in den Trainingsdaten Muster gelernt hat, die im realen Sprachgebrauch nicht', metadata={'source': '/Users/swmoeller/python/2023/large_language_model/BrainGPT/data/10_raw/doc2scan/Regulierung_von_Large_Language_Models_in_DSA_und_AIA-E_-_beck-online.pdf'}), Document(page_content='Redaktion MMR-Aktuell\\n\\nKurzbeiträge/Kommentare\\n\\nMMR-Aktuell 2023, 455171\\n\\nRegulierung von Large Language Models in DSA und AIA-E\\n\\nAlexander Wehde ist studentische Hilfskraft am Lehrstuhl für Bürgerliches Recht, Informations- und \\n\\nDatenrecht bei Prof. Dr. Louisa Specht-Riemenschneider an der Rheinischen-Friedrich-Wilhelms-\\n\\nUniversität Bonn und stellv. Vorstandsvorsitzender der Forschungsstelle für Rechtsfragen neuer \\n\\nTechnologien sowie Datenrecht (ForTech) e.V.\\n\\nDie Regulierung großer Sprachmodelle (engl. Large Language Models) ist diffizil, befindet sie sich \\n\\ndoch im Spannungsfeld von Innovationsförderung und mannigfaltigen Befürchtungen über die \\n\\ngenaue Verwendung sowie Ausgestaltung solcher Systeme. Letztere erwachsen dabei insbesondere \\n\\naufgrund der Tatsache, dass Large Language Models in der Lage sind, Text zu erzeugen, der dem \\n\\nLeser den Eindruck vermitteln mag von Menschen geschriebener Text läge vor (sog. ELIZA-Effekt).', metadata={'source': '/Users/swmoeller/python/2023/large_language_model/BrainGPT/data/10_raw/doc2scan/Regulierung_von_Large_Language_Models_in_DSA_und_AIA-E_-_beck-online.pdf'}), Document(page_content='für die Verarbeitung von Sprach- oder Texteingaben verwendet, wie zB im Rahmen maschineller \\n\\nÜbersetzungen oder zur Sprach- und Textgenerierung.\\n\\nTechnisch arbeiten Large Language Models häufig auf Basis sog. Feedforward Networks, die als \\n\\nUnterform künstlicher neuronaler Netzwerke beschrieben werden können (zur Architekur von Large \\n\\nLanguage Models etwa Vaswani et al., Attention Is All You Need, NIPS 2017). Als solche orientieren \\n\\nsich künstliche neuronale Netzwerke grundsätzlich am Erkenntnisstand der Arbeitsweise des \\n\\nmenschlichen Gehirns und bestehen aus miteinander verbundenen künstlichen Neuronen, die Daten \\n\\nverarbeiten und verknüpfen können. Im Kontext von Large Language Models sind diese im \\n\\nkünstlichen neuronalen Netzwerk in unterschiedlichen bzw. mehreren „Layern“ organisiert, wobei \\n\\nder Input Layer die rohen Eingabedaten des Nutzenden erhält und der Output Layer die \\n\\nAusgabevorhersage erstellt. Zwischen dem Input- und Output Layer liegen mehrere sog. Hidden', metadata={'source': '/Users/swmoeller/python/2023/large_language_model/BrainGPT/data/10_raw/doc2scan/Regulierung_von_Large_Language_Models_in_DSA_und_AIA-E_-_beck-online.pdf'}), Document(page_content='Vor diesem Hintergrund stellt sich die Frage, inwieweit die angestrebte bzw. bereits verankerte \\n\\ndigitalpolitische Regulierung in Gestalt des Digital Services Act (DSA) und des Komissionsentwurfs \\n\\nzum Artificial Intelligence Act (AIA-E) Large Language Models angemessen berücksichtigen, um \\n\\neinen ethischen und verantwortungsbewussten Einsatz dieser in Zukunft sicherzustellen.\\n\\nI. Large Language Models\\n\\nLarge Language Models wie GPT-3, BERT oder Blender Bot 3 erfreuen sich dieser Tage großer \\n\\nAufmerksamkeit. Die Bezeichnung dieser als „large“ stellt dabei einen Rückbezug auf den sehr \\n\\ngroßen Datensatz anhand dessen das System trainiert wurde dar. Jener besteht oftmals aus \\n\\nMilliarden von Wörtern und Abfolgen dieser, was es den Large Language Models ermöglicht die \\n\\nMuster und Strukturen menschlicher Sprache verblüffend präzise zu erlernen und Text zu \\n\\ngenerieren, der dem eines Menschen ähnlich ist. Heute werden große Sprachmodelle daher häufig', metadata={'source': '/Users/swmoeller/python/2023/large_language_model/BrainGPT/data/10_raw/doc2scan/Regulierung_von_Large_Language_Models_in_DSA_und_AIA-E_-_beck-online.pdf'})]}\n",
    "\n",
    "\n",
    "extracted_data = []\n",
    "#Create list of all items page_content, metadata, score\n",
    "for item in your_data:\n",
    "    document_string = item[0]\n",
    "    \n",
    "    content_start = document_string.find(\"page_content='\") + len(\"page_content='\")\n",
    "    content_end = document_string.find(\"'\", content_start)\n",
    "    page_content = document_string[content_start:content_end]\n",
    "    \n",
    "    metadata_start = document_string.find(\"metadata=\") + len(\"metadata=\")\n",
    "    metadata_end = document_string.find(\"})\", metadata_start) + 1 \n",
    "    metadata_str = document_string[metadata_start:metadata_end]\n",
    "    \n",
    "    metadata = eval(metadata_str)\n",
    "    \n",
    "    score = item[1]\n",
    "    \n",
    "    extracted_data.append({\n",
    "        'page_content': page_content,\n",
    "        'metadata': metadata,\n",
    "        'score': score\n",
    "    })\n",
    "#Iterate over list\n",
    "for item in extracted_data:\n",
    "    print(item['page_content'])\n",
    "    print(item['metadata'])\n",
    "    print(item['score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': 'Question string', 'chat_history': [], 'answer': ' Answer string', 'source_documents': [{'page_content'='Extract from book','metadata'={'source': 'path'}},{'page_content'='Extract from book','metadata'={'source': 'path'}}, {'page_content'='Extract from book', 'metadata'={'source': 'path'}}]}\n"
     ]
    },
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting property name enclosed in double quotes: line 1 column 2 (char 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m/Users/swmoeller/python/2023/large_language_model/BrainGPT/scripts/trials/decompose_answer.ipynb Cell 4\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/swmoeller/python/2023/large_language_model/BrainGPT/scripts/trials/decompose_answer.ipynb#W4sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m data \u001b[39m=\u001b[39m re\u001b[39m.\u001b[39msub(\u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\\\u001b[39m\u001b[39m}\u001b[39m\u001b[39m\\\u001b[39m\u001b[39m)\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m}}\u001b[39m\u001b[39m\"\u001b[39m, data)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/swmoeller/python/2023/large_language_model/BrainGPT/scripts/trials/decompose_answer.ipynb#W4sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39mprint\u001b[39m(data)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/swmoeller/python/2023/large_language_model/BrainGPT/scripts/trials/decompose_answer.ipynb#W4sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m data_try \u001b[39m=\u001b[39m json\u001b[39m.\u001b[39;49mloads(data)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/swmoeller/python/2023/large_language_model/BrainGPT/scripts/trials/decompose_answer.ipynb#W4sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m question \u001b[39m=\u001b[39m data_try[\u001b[39m\"\u001b[39m\u001b[39mquestion\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/swmoeller/python/2023/large_language_model/BrainGPT/scripts/trials/decompose_answer.ipynb#W4sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m data_json \u001b[39m=\u001b[39m json\u001b[39m.\u001b[39mdumps(data, indent\u001b[39m=\u001b[39m\u001b[39m4\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/braingpt/lib/python3.11/json/__init__.py:346\u001b[0m, in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    341\u001b[0m     s \u001b[39m=\u001b[39m s\u001b[39m.\u001b[39mdecode(detect_encoding(s), \u001b[39m'\u001b[39m\u001b[39msurrogatepass\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    343\u001b[0m \u001b[39mif\u001b[39;00m (\u001b[39mcls\u001b[39m \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m object_hook \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m\n\u001b[1;32m    344\u001b[0m         parse_int \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m parse_float \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m\n\u001b[1;32m    345\u001b[0m         parse_constant \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m object_pairs_hook \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m kw):\n\u001b[0;32m--> 346\u001b[0m     \u001b[39mreturn\u001b[39;00m _default_decoder\u001b[39m.\u001b[39;49mdecode(s)\n\u001b[1;32m    347\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mcls\u001b[39m \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    348\u001b[0m     \u001b[39mcls\u001b[39m \u001b[39m=\u001b[39m JSONDecoder\n",
      "File \u001b[0;32m~/anaconda3/envs/braingpt/lib/python3.11/json/decoder.py:337\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecode\u001b[39m(\u001b[39mself\u001b[39m, s, _w\u001b[39m=\u001b[39mWHITESPACE\u001b[39m.\u001b[39mmatch):\n\u001b[1;32m    333\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[1;32m    334\u001b[0m \u001b[39m    containing a JSON document).\u001b[39;00m\n\u001b[1;32m    335\u001b[0m \n\u001b[1;32m    336\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 337\u001b[0m     obj, end \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mraw_decode(s, idx\u001b[39m=\u001b[39;49m_w(s, \u001b[39m0\u001b[39;49m)\u001b[39m.\u001b[39;49mend())\n\u001b[1;32m    338\u001b[0m     end \u001b[39m=\u001b[39m _w(s, end)\u001b[39m.\u001b[39mend()\n\u001b[1;32m    339\u001b[0m     \u001b[39mif\u001b[39;00m end \u001b[39m!=\u001b[39m \u001b[39mlen\u001b[39m(s):\n",
      "File \u001b[0;32m~/anaconda3/envs/braingpt/lib/python3.11/json/decoder.py:353\u001b[0m, in \u001b[0;36mJSONDecoder.raw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Decode a JSON document from ``s`` (a ``str`` beginning with\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[39ma JSON document) and return a 2-tuple of the Python\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[39mrepresentation and the index in ``s`` where the document ended.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    350\u001b[0m \n\u001b[1;32m    351\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    352\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 353\u001b[0m     obj, end \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mscan_once(s, idx)\n\u001b[1;32m    354\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m    355\u001b[0m     \u001b[39mraise\u001b[39;00m JSONDecodeError(\u001b[39m\"\u001b[39m\u001b[39mExpecting value\u001b[39m\u001b[39m\"\u001b[39m, s, err\u001b[39m.\u001b[39mvalue) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def print_nested_dict(d, indent=0):\n",
    "    for key, value in d.items():\n",
    "        if isinstance(value, dict):\n",
    "            print(\" \" * indent + f\"{key}:\")\n",
    "            print_nested_dict(value, indent + 4)\n",
    "        else:\n",
    "            print(\" \" * indent + f\"{key}: {value}\")\n",
    "\n",
    "\n",
    "data = '''{'question': 'Question string', 'chat_history': [], 'answer': ' Answer string', 'source_documents': [Document(page_content='Extract from book',metadata={'source': 'path'}),Document(page_content='Extract from book',metadata={'source': 'path'}), Document(page_content='Extract from book', metadata={'source': 'path'})]}'''\n",
    "\n",
    "# Replace \"Document(page_content=\" with \"'Document'('page_content'=\" and \"metadata=\" with \"'metadata'=\"\n",
    "data = re.sub(r'\\[Document\\(page_content=', \"[{'page_content'=\", data)\n",
    "data = re.sub(r'Document\\(page_content=', \"{'page_content'=\", data)\n",
    "data = re.sub(r'metadata=', \"'metadata'=\", data)\n",
    "data = re.sub(r\"'\\}\\)\", \"'}}\", data)\n",
    "\n",
    "print(data)\n",
    "data_try = json.loads(data)\n",
    "question = data_try[\"question\"]\n",
    "\n",
    "\n",
    "data_json = json.dumps(data, indent=4)\n",
    "data_cleaned = json.loads(data_json)\n",
    "\n",
    "#print_nested_dict(data)\n",
    "# Get values for 'question', 'chat_history', and 'answer'\n",
    "question = data_cleaned[\"question\"]\n",
    "chat_history = data_cleaned[\"chat_history\"]\n",
    "answer = data_cleaned[\"answer\"]\n",
    "\n",
    "# Generate tuples for 'page_content' and its source for each source document\n",
    "page_content_sources = [(doc['page_content'], doc['metadata']['source']) for doc in data_cleaned['source_documents']]\n",
    "\n",
    "# Print the results\n",
    "print(\"Question:\", question)\n",
    "print(\"Chat History:\", chat_history)\n",
    "print(\"Answer:\", answer)\n",
    "print(\"Page Content and Sources:\")\n",
    "for page_content, source in page_content_sources:\n",
    "    print(f\"  Page Content: {page_content}\")\n",
    "    print(f\"  Source: {source}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "braingpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
