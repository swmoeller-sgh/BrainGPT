{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# [IMPORT of modules and packages]\n",
    "import os\n",
    "import logging\n",
    "\n",
    "import openai\n",
    "\n",
    "from langchain.llms.openai import OpenAI\n",
    "from langchain.vectorstores.chroma import Chroma\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "from chromadb.config import Settings\n",
    "\n",
    "env_file = find_dotenv(\".env\")\n",
    "load_dotenv(env_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition of variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "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\n"
     ]
    }
   ],
   "source": [
    "# [DIRECTORY definition]\n",
    "ROOT_DIR = os.path.join(str(os.getenv(\"ROOT_DIR\")))\n",
    "BRAINGPT_GENERATE_ANSWER_LOG_FILE = os.path.join(ROOT_DIR, str(os.getenv(\"LOG_DIR\")),\n",
    "                                     str(os.getenv(\"QUERY_LOG_NAME\")))\n",
    "PROCESSED_DATA_DIR = os.path.join(ROOT_DIR,\n",
    "                                  str(os.getenv(\"PROCESSED_DATA_DIR\")),\n",
    "                                  str(os.getenv(\"CHROMA_DB\")))\n",
    "\n",
    "# [LOAD enironment variables]\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")        # openAI-settings\n",
    "\n",
    "# Define the Chroma settings\n",
    "CHROMA_SETTINGS = Settings(\n",
    "        persist_directory=PROCESSED_DATA_DIR,\n",
    "        anonymized_telemetry=False)\n",
    "\n",
    "\n",
    "# CONFIGURATION\n",
    "USED_MODEL = \"OPENAI\"\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "vectordb = Chroma(persist_directory=PROCESSED_DATA_DIR,\n",
    "                  embedding_function=embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.pydantic_v1 import BaseModel, Field, validator\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qa_chained_history_parsed_output(in__question:str,                  #pylint: disable=W0102\n",
    "                             in__llm: object,       # Default value for Language Model object\n",
    "                             in__chat_history: list = [],\n",
    "                             in__vectordb: Chroma =vectordb):    # Default value for VectorDB object\n",
    "    \"\"\"\n",
    "    Receive a question, a context (chat history) with used vectorstore and generate \n",
    "    an answer including the document source\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    in_question : str\n",
    "        question being raised\n",
    "    in_llm: object\n",
    "        used language model\n",
    "    in_chat_history: list of tuple\n",
    "        previous questions and their answers\n",
    "    in_vectordb : \n",
    "        vectorstore\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    json\n",
    "        JSON object containing question and result including the answer and the source document\n",
    "    \"\"\"\n",
    "    json_schema = {\n",
    "        \"question\": \"Provided question without change\",\n",
    "        \"answer\": \"Answer for the question.\",\n",
    "        \"metadata\": \"source\": {\"citation\": \"An extract from the source in its original language. Limited to 20 words\", \n",
    "                       \"path\": \"The path to the original file\", \n",
    "                       \"type\": \"string\"},\n",
    "        },\n",
    "        \"required\": [\"name\", \"age\"],\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    return result   # Return the JSON response object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    llm_temp0 = OpenAI(temperature=0)       #type: ignore\n",
    "\n",
    "    response = qa_chained_history_parsed_output(in__question=\"How to describe VSM in three bullet points?\",\n",
    "                                        in__llm=llm_temp0,\n",
    "                                        in__vectordb=vectordb)\n",
    "    print(response)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "braingpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
