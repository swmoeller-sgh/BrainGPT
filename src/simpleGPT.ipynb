{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Purpose\n",
    "Using local (private) documents in different formats, a chatbot answers questions (in a dialogue). Later, local hosting and extension towards PA is planned. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Open Point List & Ideas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OPL\n",
    "- include reference\n",
    "\n",
    "## Ideas \n",
    "- Change from OpenAI to another model\n",
    "- Establish dialogue instead of question only\n",
    "- increase document base\n",
    "- involve OpenAI in case no local answer is found (e.g. based on similarity in vectorstore) - avoid hallucination"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set the project environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import all packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "import uuid\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from typing import List\n",
    "import magic\n",
    "\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import VectorDBQA\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.document_loaders import TextLoader\n",
    "\n",
    "from chromadb.config import Settings\n",
    "\n",
    "\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "from constants import CHROMA_SETTINGS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import (\n",
    "    CSVLoader,\n",
    "    EverNoteLoader,\n",
    "    PDFMinerLoader,\n",
    "    TextLoader,\n",
    "    UnstructuredEmailLoader,\n",
    "    UnstructuredEPubLoader,\n",
    "    UnstructuredHTMLLoader,\n",
    "    UnstructuredMarkdownLoader,\n",
    "    UnstructuredODTLoader,\n",
    "    UnstructuredPowerPointLoader,\n",
    "    UnstructuredWordDocumentLoader,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOADER_MAPPING = {\n",
    "    \".csv\": (CSVLoader, {}),\n",
    "    # \".docx\": (Docx2txtLoader, {}),\n",
    "    \".doc\": (UnstructuredWordDocumentLoader, {}),\n",
    " #   \".docx\": (UnstructuredWordDocumentLoader, {}),\n",
    "    \".enex\": (EverNoteLoader, {}),\n",
    " #   \".eml\": (MyElmLoader, {}),\n",
    "    \".epub\": (UnstructuredEPubLoader, {}),\n",
    "    \".html\": (UnstructuredHTMLLoader, {}),\n",
    "    \".md\": (UnstructuredMarkdownLoader, {}),\n",
    "    \".odt\": (UnstructuredODTLoader, {}),\n",
    "    \".pdf\": (PDFMinerLoader, {}),\n",
    "    \".ppt\": (UnstructuredPowerPointLoader, {}),\n",
    "    \".pptx\": (UnstructuredPowerPointLoader, {}),\n",
    "    \".txt\": (TextLoader, {\"encoding\": \"utf8\"}),\n",
    "    # Add more mappings for other file extensions and loaders as needed\n",
    "}\n",
    "\n",
    "# determine available loaders based on their extension\n",
    "available_loader_types = [ext.lstrip(\".\") for ext in LOADER_MAPPING.keys()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load environmental variables\n",
    "load_dotenv()\n",
    "\n",
    "# directory definition\n",
    "output_directory = os.environ.get(\"OUTPUT_DIRECTORY\")\n",
    "doc_source = os.environ.get(\"DOC_SOURCE_DIRECTORY\")\n",
    "persist_directory = os.environ.get('SIMPLEGPT_PERSIST_DIRECTORY')\n",
    "embeddings_model_name = os.environ.get('EMBEDDINGS_MODEL_NAME')\n",
    "\n",
    "# Define the Chroma settings\n",
    "chroma_settings = Settings(chroma_db_impl='duckdb+parquet',persist_directory=persist_directory,anonymized_telemetry=False)\n",
    "\n",
    "# import tracking file\n",
    "import_tracking_file = os.path.join(persist_directory, \"simpleGPT_import_tracking.csv\")\n",
    "\n",
    "used_model = \"OPENAI\"   \n",
    "# used_model = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definition of essential functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate a list of all files in one directory and its subdirectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_file_paths(directory_path):\n",
    "    file_paths = []\n",
    "    for root, _, files in os.walk(directory_path):\n",
    "        for file in files:\n",
    "            file_path = os.path.join(root, file)\n",
    "            file_paths.append(file_path)\n",
    "    return file_paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Database to track already imported documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Check, if data-file with list of imported files and their uuid already exists\n",
    "def open_import_tracking(IN_tracking_file:str):\n",
    "    \n",
    "    if os.path.isfile(IN_tracking_file):\n",
    "        print(\"[LOADING...] import tracking file.\")\n",
    "        import_tracking_df = pd.read_csv(IN_tracking_file)\n",
    "    else:\n",
    "        print(\"[MISSING] import tracking file. No worries, will be generated later automatically!\")\n",
    "        columns = [\"uuid\", \"file_path\",\"type of file\"]\n",
    "        import_tracking_df = pd.DataFrame(columns=columns)\n",
    "    return import_tracking_df\n",
    "\n",
    "\n",
    "# generate an uuid for a document\n",
    "def get_uuid5(IN_file):\n",
    "    unique_id = str(uuid.uuid5(uuid.NAMESPACE_DNS, IN_file))\n",
    "    return(unique_id)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save dataframe to disk\n",
    "def save_dataframe_to_file(dataframe, filename):    \n",
    "    dataframe.to_csv(filename, index=False)\n",
    "    print(f\"[SUCCESS] Dataframe saved to {filename}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read a file using the defined document loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_single_file(link_to_file: str):\n",
    "    ext = \".\" + link_to_file.rsplit(\".\", 1)[-1]\n",
    "\n",
    "    document = []\n",
    "    if ext in LOADER_MAPPING:\n",
    "        # generate unique id from document and save both the name and unique ID into the pandas dataframe\n",
    "        uuid_value = get_uuid5(link_to_file)\n",
    "\n",
    "        global import_tracking_df\n",
    "        if uuid_value in import_tracking_df[\"uuid\"].values:\n",
    "            print(f\"[SKIPPED] Document {link_to_file} already exists in import-record, hence in database. Skipped.\")\n",
    "\n",
    "        else:\n",
    "            new_row_df = pd.DataFrame({\"file_path\": [link_to_file], \"uuid\": [uuid_value], \"type of file\":[type(link_to_file)]})\n",
    "            import_tracking_df = pd.concat([new_row_df, import_tracking_df], ignore_index=True)\n",
    "\n",
    "            # load the document\n",
    "            loader_class, loader_args = LOADER_MAPPING[ext]\n",
    "            loader = loader_class(link_to_file, **loader_args)\n",
    "            document = loader.load()\n",
    "            print(f\"[INFO] Document loaded: {link_to_file}.\")\n",
    "\n",
    "    else:\n",
    "        print(f\"[ERROR] Document NOT loaded: {link_to_file} with extension {ext} skipped due to unknown extension.\")\n",
    "        \n",
    "    return document\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load all documents from a list containing the full path to files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_all_documents(IN_doc_source: str):\n",
    "    list_of_files = get_all_file_paths(IN_doc_source)\n",
    "    result = []\n",
    "\n",
    "    for single_file in list_of_files:\n",
    "        result.extend(read_single_file(single_file))\n",
    "        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split documents into chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_doc_into_chunks(in_documents):\n",
    "\n",
    "    texts = []\n",
    "    if len(in_documents) != 0:\n",
    "        # Split the documents\n",
    "        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "        texts = text_splitter.split_documents(in_documents)\n",
    "    else:\n",
    "        print(\"[ERROR] No documents loaded!\")\n",
    "        \n",
    "    return(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorstore processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check, if vectorstock exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def does_vectorstore_exist(persist_directory: str) -> bool:\n",
    "    \"\"\"\n",
    "    Checks if vectorstore exists\n",
    "    \"\"\"\n",
    "    if os.path.exists(os.path.join(persist_directory, 'index')):\n",
    "        if os.path.exists(os.path.join(persist_directory, 'chroma-collections.parquet')) and os.path.exists(os.path.join(persist_directory, 'chroma-embeddings.parquet')):\n",
    "            list_index_files = os.path.join(persist_directory, 'index/*.bin')\n",
    "            list_index_files += os.path.join(persist_directory, 'index/*.pkl')\n",
    "            # At least 3 documents are needed in a working vectorstore\n",
    "            if len(list_index_files) > 3:\n",
    "                return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAIN part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unable to connect optimized C data functions [No module named 'clickhouse_connect.driverc.buffer'], falling back to pure Python\n",
      "Unable to connect ClickHouse Connect C to Numpy API [No module named 'clickhouse_connect.driverc.npconv'], falling back to pure Python\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOADING...] import tracking file.\n",
      "[SKIPPED] Document /Users/swmoeller/python/2023/NLP/BrainGPT/data/00_doc2scan/Die Dritte Dimension - Martin Pfiffner.pdf already exists in import-record, hence in database. Skipped.\n",
      "[ERROR] Document NOT loaded: /Users/swmoeller/python/2023/NLP/BrainGPT/data/00_doc2scan/No ETA Maverick 400 Specifications.docx with extension .docx skipped due to unknown extension.\n",
      "[SKIPPED] Document /Users/swmoeller/python/2023/NLP/BrainGPT/data/00_doc2scan/Bosch_Washing_Machine.pdf already exists in import-record, hence in database. Skipped.\n",
      "[SKIPPED] Document /Users/swmoeller/python/2023/NLP/BrainGPT/data/00_doc2scan/state_of_the_union copy.txt already exists in import-record, hence in database. Skipped.\n",
      "[SKIPPED] Document /Users/swmoeller/python/2023/NLP/BrainGPT/data/00_doc2scan/state_of_the_union.txt already exists in import-record, hence in database. Skipped.\n",
      "[SKIPPED] Document /Users/swmoeller/python/2023/NLP/BrainGPT/data/00_doc2scan/Regulierung_von_Large_Language_Models_in_DSA_und_AIA-E_-_beck-online.pdf already exists in import-record, hence in database. Skipped.\n",
      "[SKIPPED] Document /Users/swmoeller/python/2023/NLP/BrainGPT/data/00_doc2scan/1. Standard notations for Deep Learning.pdf already exists in import-record, hence in database. Skipped.\n",
      "[SKIPPED] Document /Users/swmoeller/python/2023/NLP/BrainGPT/data/00_doc2scan/subfolder/README.md already exists in import-record, hence in database. Skipped.\n",
      "[SKIPPED] Document /Users/swmoeller/python/2023/NLP/BrainGPT/data/00_doc2scan/subfolder/Yacht Magazin vom 20.12.2006.pdf already exists in import-record, hence in database. Skipped.\n",
      "[INFO] 0 documents were loaded\n",
      "[ERROR] No documents loaded!\n",
      "[INFO] Number of chunks 0\n",
      "[INFO] A vectorstore exists. I will append to this one!\n",
      "[SUCCESS] Dataframe saved to /Users/swmoeller/python/2023/NLP/BrainGPT/data/25_output/SimpleGPT/simpleGPT_import_tracking.csv.\n",
      "[INFO] Query\n",
      "How do you change an organization?\n",
      "[INFO] Result\n",
      " You can change an organization by either incrementally making changes over time or by reorganizing it. Reorganization involves putting new structures and processes in place and can help create better conditions for knowledge workers in a dynamic environment.\n",
      "\n",
      "[INFO] Source\n",
      "[Document(page_content='Inkrementelle Veränderung oder Reorganisation?\\n\\n271\\n\\nDa ein Umbau nicht wie bei einem Flugzeug in der Wartungshalle vorgenommen werden \\nkann, sondern bei voller Flughöhe und Fluggeschwindigkeit, ist das eine anspruchsvolle \\nAufgabe, mit der die wenigsten Führungskräfte wirklich nutzbare Erfahrung haben. Sie \\nhaben im Laufe ihrer Karriere vielleicht zwei oder drei größere Reorganisationen miter-\\nlebt,  vielleicht  eine  davon  selber  verantwortet,  und  die  letzte  Erfahrung  liegt  bestimmt \\nschon Jahre zurück.', metadata={'source': '/Users/swmoeller/python/2023/NLP/BrainGPT/data/00_doc2scan/Die Dritte Dimension - Martin Pfiffner.pdf'}), Document(page_content='bei jeder Verunsicherung in das Alte zurück flüchten. Wer Neues einführen will, muss \\ndazu das Alte definitiv aus der Welt schaffen. Auch wenn zum Zeitpunkt der Scharf-\\nstellung der neuen Organisation noch vieles offen bleiben muss, kann sie doch gestar-\\ntet werden. Die neuen Verantwortlichen sollen dadurch in den Fahrersitz gehoben \\nwerden, damit sie die verbleibenden Lücken während der folgenden sechsmonatigen \\nImplementierungsphase  schließen.  Es  ist  die  Zeit,  in  der  detailliert  ausprobiert,  ge-\\nlernt, feinjustiert und korrigiert wird. Nach diesen sechs Monaten sollte das Umset-\\nzungsprojekt abgeschlossen werden. Für alle noch offenen Punkte gelten ab dann die \\nnormalen Entscheidungswege in der neuen Organisation. Andernfalls werden aus Re-\\norganisationsprojekten endlose Exkursionen.', metadata={'source': '/Users/swmoeller/python/2023/NLP/BrainGPT/data/00_doc2scan/Die Dritte Dimension - Martin Pfiffner.pdf'}), Document(page_content='17.1 \\n\\n Inkrementelle Veränderung oder Reorganisation?', metadata={'source': '/Users/swmoeller/python/2023/NLP/BrainGPT/data/00_doc2scan/Die Dritte Dimension - Martin Pfiffner.pdf'}), Document(page_content='erleichtern,  wenn  sie  richtig  gestaltet  sind.  Die  Organisation  rückt  deshalb  seit  einigen \\nJahren wieder ins Interesse der Führungskräfte, und wir erleben derzeit so etwas wie eine \\nWiedergeburt des Organisierens [9]. Man beginnt nach neuen Lösungen zu suchen, und so \\ntreiben bald neue Organisationsformen als aktuelle Modewellen durch die Wirtschaft. \\nSie versuchen bessere Rahmenbedingungen für den Wissensarbeiter im dynamischen Um-\\nfeld zu schaffen und enthalten die immer wieder neu aufflackernde und ideologisch be-\\nsetzte Idee der Hierarchiefreiheit, und damit das ungelöste Problem von Autonomie versus \\nKohäsion.', metadata={'source': '/Users/swmoeller/python/2023/NLP/BrainGPT/data/00_doc2scan/Die Dritte Dimension - Martin Pfiffner.pdf'})]\n"
     ]
    }
   ],
   "source": [
    "# initializing the vectorstore\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# check, if import_tracking_file exists\n",
    "import_tracking_df = open_import_tracking(IN_tracking_file= import_tracking_file)\n",
    "\n",
    "documents = load_all_documents(doc_source)\n",
    "print(f\"[INFO] {len(documents)} documents were loaded\")\n",
    "\n",
    "\n",
    "text_chunks = split_doc_into_chunks(documents)\n",
    "print(f\"[INFO] Number of chunks {len(text_chunks)}\")\n",
    "\n",
    "\n",
    "# check, if the vectorstore exists\n",
    "if does_vectorstore_exist(persist_directory=persist_directory) is True:\n",
    "    print(\"[INFO] A vectorstore exists. I will append to this one!\")\n",
    "\n",
    "    # loading the vectorstore\n",
    "    vectordb = Chroma(persist_directory=persist_directory, \n",
    "                      embedding_function=embeddings,\n",
    "                      client_settings=chroma_settings)\n",
    "    \n",
    "    # adding documents\n",
    "    vectordb.add_documents(text_chunks)\n",
    "\n",
    "else:\n",
    "    print(\"[INFO] No vectorstore exists. I will create a new one for you!\")\n",
    "    vectordb = Chroma.from_documents(documents=text_chunks, \n",
    "                               embedding=embeddings, \n",
    "                               persist_directory=persist_directory, \n",
    "                               client_settings=chroma_settings)\n",
    "\n",
    "# saving the vectorstore\n",
    "vectordb.persist()\n",
    "vectordb = None\n",
    "\n",
    "# save import log\n",
    "save_dataframe_to_file(dataframe=import_tracking_df,\n",
    "                       filename=import_tracking_file)\n",
    "\n",
    "# loading the vectorstore\n",
    "vectordb = Chroma(persist_directory=persist_directory,\n",
    "                   embedding_function=embeddings)\n",
    "\n",
    "# Create retriever and the chain\n",
    "qa = RetrievalQA.from_chain_type(llm=OpenAI(), chain_type=\"stuff\", retriever=vectordb.as_retriever(), return_source_documents=True)\n",
    "\n",
    "query = \"How do you change an organization?\"\n",
    "# qa.run(query)\n",
    "\n",
    "\n",
    "\n",
    "result = qa({\"query\": query})\n",
    "\n",
    "#print(qa)\n",
    "\n",
    "print(\"[INFO] Query\")\n",
    "print(result[\"query\"])\n",
    "\n",
    "print(\"[INFO] Result\")\n",
    "print(result[\"result\"])\n",
    "\n",
    "print(\"\\n[INFO] Source\")\n",
    "print(result[\"source_documents\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "brainGPT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
