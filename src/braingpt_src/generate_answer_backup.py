""" Module providing question - answer functions using an existing vectorstore"""

# [IMPORT of modules and packages]
import os
import logging

import openai

from dotenv import load_dotenv, find_dotenv

# // TODO Neuinstallation env: langchain.vectorstores --> langchain.vectorstores.chroma wechseln
from langchain.llms.openai import OpenAI
from langchain.vectorstores.chroma import Chroma
from langchain.embeddings import OpenAIEmbeddings
from langchain.chains import RetrievalQA, ConversationalRetrievalChain, RetrievalQAWithSourcesChain
from langchain.memory import ConversationBufferMemory
from langchain.prompts.prompt import PromptTemplate
from langchain.output_parsers.json import SimpleJsonOutputParser

from langchain.output_parsers import StructuredOutputParser, ResponseSchema
from langchain.prompts import PromptTemplate, ChatPromptTemplate, HumanMessagePromptTemplate
from langchain.chat_models import ChatOpenAI

from langchain.retrievers.multi_query import MultiQueryRetriever

from chromadb.config import Settings

#pylint: disable=E0401

from braingpt_config import config_braingpt # type: ignore


# [INITIALIZE environment]
env_file = find_dotenv(".env")
load_dotenv(env_file)

# [DIRECTORY definition]
ROOT_DIR = os.path.join(str(os.getenv("ROOT_DIR")))
BRAINGPT_GENERATE_ANSWER_LOG_FILE = os.path.join(ROOT_DIR, str(os.getenv("LOG_DIR")),
                                     str(os.getenv("QUERY_LOG_NAME")))


config_braingpt.setup_logging(in__log_path_file=BRAINGPT_GENERATE_ANSWER_LOG_FILE)

# [LOAD enironment variables]
openai.api_key = os.getenv("OPENAI_API_KEY")        # openAI-settings

#output_directory = os.environ.get("OUTPUT_DIRECTORY")
#doc_source = os.environ.get("DOC_SOURCE_DIRECTORY")

PROCESSED_DATA_DIR = os.path.join(ROOT_DIR,
                                  str(os.getenv("PROCESSED_DATA_DIR")),
                                  str(os.getenv("CHROMA_DB")))
logging.info("Target directory for processed files: %s", PROCESSED_DATA_DIR)

# PERSIST_DIRECTORY = str(os.environ.get('SIMPLEGPT_PERSIST_DIRECTORY'))

# LLM settings
#embeddings_model_name = os.environ.get('EMBEDDINGS_MODEL_NAME')

# Define the Chroma settings
CHROMA_SETTINGS = Settings(
        persist_directory=PROCESSED_DATA_DIR,
        anonymized_telemetry=False
)


# CONFIGURATION
USED_MODEL = "OPENAI"

embeddings = OpenAIEmbeddings() # type: ignore
vectordb = Chroma(persist_directory=PROCESSED_DATA_DIR,
                  embedding_function=embeddings)





# FUNCTIONS TO PROCESS QUESTIONS AND DERIVE ANSWERS #

def process_question_isolated(in__question: str,
                              in__llm: object,
                              in__vectordb=vectordb):
    """
    Receive a question with used vectorstore and generate an answer including the document source

    Parameters
    ----------
    in_question : str
        question being raised
    in_vectordb : _type_
        vectorstore
    """
    # Load the vectorstore
    # Creating a RetrievalQA object for question answering
    question_answer = RetrievalQA.from_chain_type(llm=in__llm,          # type: ignore
                                                  chain_type="stuff",
                                                  retriever = in__vectordb.as_retriever(),
                                                  return_source_documents=True)

    # Get the result from the QA model using the input question
    result = question_answer({"question": in__question})  # receive answer & associated source doc

    # Create the JSON response object
    total_response = {
        "question": result["question"], # The original question
        "result": {
            "answer": result["answer"], # The answer generated by the model
            "source_documents": result["source_documents"] # List of source docs. rel. to  answer
        }
    }
    return total_response # List of source documents relevant to the answer


def process_question_chained(in__question:str,                  #pylint: disable=W0102
                             in__llm: object,       # Default value for Language Model object
                             in__chat_history: list = [],
                             in__vectordb: Chroma =vectordb):    # Default value for VectorDB object
    """
    Receive a question, a context (chat history) with used vectorstore and generate 
    an answer including the document source

    Parameters
    ----------
    in_question : str
        question being raised
    in_llm: object
        used language model
    in_chat_history: list of tuple
        previous questions and their answers
    in_vectordb : 
        vectorstore

    Returns
    -------
    json
        JSON object containing question and result including the answer and the source document
    """

    from langchain.prompts import (
    PromptTemplate,
    ChatPromptTemplate,
    HumanMessagePromptTemplate,
    )

    from langchain.output_parsers import PydanticOutputParser
    from pydantic import BaseModel, Field, validator
    from typing import List, Dict, TypedDict


    class answer_source(BaseModel):
        question: str = Field(description="The stated question.")
        answer: str = Field(description="Answer to the user's question")
 
    parser = PydanticOutputParser(pydantic_object=answer_source)
    format_instructions = parser.get_format_instructions()

    prompt = PromptTemplate(
        template="answer the users question as best as possible.\n{format_instructions}\n{question}",
        input_variables=["question"],
        partial_variables={"format_instructions": format_instructions}
        )


    question_answer = ConversationalRetrievalChain.from_llm(llm=in__llm,    # type: ignore
                                                            # VectorDB object converted to retriever
                                                            retriever=in__vectordb.as_retriever(),
                                                            # verbose informmation
                                                            verbose=True,
                                                            # incl source doc
                                                            return_source_documents=True,
                                                            chain_type="stuff")




    # Query the ConversationalRetrievalChain with the given question and chat history
    _input = prompt.format_prompt(question=in__question)

    result = question_answer({"question": _input.to_string(),
                              "chat_history": in__chat_history})

    output_parser.parse(result)


    logging.info(result)
    '''
    # Create the JSON response object
    total_response = {
        "question": result["question"],
        "result": {
            "answer": result["answer"],
            "source_documents": result["source_documents"]  # Include source doc in the response
        }
    }
    '''

    print(result)
    return result   # Return the JSON response object


def process_question_chained_memorybuffer(in__question:str,
                                          in__llm: object,
                                          in__vectordb=vectordb):
    """
    Receive a question with used vectorstore and generate an answer including the document source

    Parameters
    ----------
    in_question : str
        question being raised
    in_vectordb : _type_
        vectorstore
    """

    # CONFIGURATION

    # Memory object, which is necessary to track the inputs/outputs and hold a conversation.
    memory = ConversationBufferMemory(memory_key="chat_history",
                                      return_messages=True,
                                      input_key="question",
                                      output_key="answer")


    question_answer = ConversationalRetrievalChain.from_llm(llm=in__llm,              # type: ignore
                                               retriever=in__vectordb.as_retriever(),
                                               verbose=True,
                                               memory=memory,
                                               return_source_documents=True)


    result = question_answer({"question": in__question})

    # Create the JSON response object
    total_response = {
        "question": result["question"],
        "result": {
            "answer": result["answer"],
            "source_documents": result["source_documents"]
        }
    }

    return total_response

'''
def multi_query_retriever(in__question:str,                  #pylint: disable=W0102
                             in__llm: object,       # Default value for Language Model object
                             in__chat_history: list = [],
                             in__vectordb: Chroma =vectordb):    # Default value for VectorDB object

    retriever_from_llm = MultiQueryRetriever.from_llm(retriever=in__vectordb.as_retriever(),
                                                      llm=in__llm)
    logging.getLogger("langchain.retrievers.multi_query").setLevel(logging.INFO)

    unique_docs = retriever_from_llm.get_relevant_documents(query=in__question)
    len(unique_docs)



    # Create the JSON response object
    total_response = {
        "question": result["question"],
        "result": {
            "answer": result["answer"],
            "source_documents": result["source_documents"]  # Include source doc in the response
        }
    }

    return total_response   # Return the JSON response object
'''


if __name__ == "__main__":
    llm_temp0 = OpenAI(temperature=0)       #type: ignore

    response = process_question_chained(in__question="How to describe VSM in three bullet points?",
                                        in__llm=llm_temp0,
                                        in__vectordb=vectordb)
    print(response)
