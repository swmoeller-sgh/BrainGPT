""" Module providing question - answer functions using an existing vectorstore"""

# [IMPORT of modules and packages]
import os
import logging

import openai

from dotenv import load_dotenv, find_dotenv

from langchain.llms import OpenAI
from langchain.vectorstores import Chroma
from langchain.embeddings import OpenAIEmbeddings
from langchain.chains import RetrievalQA, ConversationalRetrievalChain
from langchain.memory import ConversationBufferMemory

from chromadb.config import Settings

#pylint: disable=E0401
from brain_gpt_config import config_braingpt # type: ignore


# [INITIALIZE environment]
env_file = find_dotenv(".env")
load_dotenv(env_file)

# [DIRECTORY definition]
ROOT_DIR = os.path.join(str(os.getenv("ROOT_DIR")))
BRAINGPT_GENERATE_ANSWER_LOG_FILE = os.path.join(ROOT_DIR, str(os.getenv("LOG_DIR")),
                                     str(os.getenv("QUERY_LOG_NAME")))


config_braingpt.setup_logging(in__log_path_file=BRAINGPT_GENERATE_ANSWER_LOG_FILE)

# [LOAD enironment variables]
openai.api_key = os.getenv("OPENAI_API_KEY")        # openAI-settings

#output_directory = os.environ.get("OUTPUT_DIRECTORY")
#doc_source = os.environ.get("DOC_SOURCE_DIRECTORY")

PROCESSED_DATA_DIR = os.path.join(ROOT_DIR, 
                                  str(os.getenv("PROCESSED_DATA_DIR")),
                                  str(os.getenv("CHROMA_DB")))
logging.info("Target directory for processed files: %s", PROCESSED_DATA_DIR)

# PERSIST_DIRECTORY = str(os.environ.get('SIMPLEGPT_PERSIST_DIRECTORY'))

# LLM settings
#embeddings_model_name = os.environ.get('EMBEDDINGS_MODEL_NAME')

# Define the Chroma settings
chroma_settings = Settings(
    chroma_db_impl='duckdb+parquet',
    persist_directory=PROCESSED_DATA_DIR,
    anonymized_telemetry=False
    )


# CONFIGURATION
USED_MODEL = "OPENAI"

embeddings = OpenAIEmbeddings() # type: ignore
vectordb = Chroma(persist_directory=PROCESSED_DATA_DIR,
                  embedding_function=embeddings)





# FUNCTIONS TO PROCESS QUESTIONS AND DERIVE ANSWERS #

def process_question_isolated(in_question:str,
                              in_llm: object,
                              in_vectordb=vectordb):
    """
    Receive a question with used vectorstore and generate an answer including the document source

    Parameters
    ----------
    in_question : str
        question being raised
    in_vectordb : _type_
        vectorstore
    """
    # Load the vectorstore
    # Creating a RetrievalQA object for question answering
    qa = RetrievalQA.from_chain_type(llm=in_llm,                            # type: ignore
                                     chain_type="stuff",
                                     retriever=in_vectordb.as_retriever(),
                                     return_source_documents=True)          # type: ignore

    # Get the result from the QA model using the input question
    result = qa({"question": in_question})  # Using the question to get an answer and associated source documents

    # Create the JSON response object
    total_response = {
        "question": result["question"], # The original question
        "result": {
            "answer": result["answer"], # The answer generated by the model
            "source_documents": result["source_documents"] # List of source documents relevant to the answer
        }
    }
    return total_response # List of source documents relevant to the answer


def process_question_chained(in__question:str,
                             in__llm: object,        # Default value for Language Model object
                             in__chat_history: list = [],
                             in__vectordb: Chroma =vectordb):    # Default value for VectorDB object
    """
    Receive a question, a context (chat history) with used vectorstore and generate an answer including the document source

    Parameters
    ----------
    in_question : str
        question being raised
    in_llm: object
        used language model
    in_chat_history: list of tuple
        previous questions and their answers
    in_vectordb : 
        vectorstore

    Returns
    -------
    json
        JSON object containing question and result including the answer and the source document
    """


    qa = ConversationalRetrievalChain.from_llm(llm=in__llm,    # Language Model object          # type: ignore
                                               retriever=in__vectordb.as_retriever(), # VectorDB object converted to retriever
                                               verbose=True,                    # Print verbose information
                                               return_source_documents=True)    # Include source documents in the result


    # Query the ConversationalRetrievalChain with the given question and chat history
    result = qa({"question": in__question,
                 "chat_history": in__chat_history})

    # Create the JSON response object
    total_response = {
        "question": result["question"],
        "result": {
            "answer": result["answer"],
            "source_documents": result["source_documents"]  # Include source documents in the response
        }
    }

    return total_response   # Return the JSON response object


def process_question_chained_memorybuffer(in__question:str,
                                          in__llm: object,
                                          in__vectordb=vectordb):
    """
    Receive a question with used vectorstore and generate an answer including the document source

    Parameters
    ----------
    in_question : str
        question being raised
    in_vectordb : _type_
        vectorstore
    """

    # CONFIGURATION

    # Memory object, which is necessary to track the inputs/outputs and hold a conversation.
    memory = ConversationBufferMemory(memory_key="chat_history",
                                      return_messages=True,
                                      input_key="question",
                                      output_key="answer")


    qa = ConversationalRetrievalChain.from_llm(llm=in__llm,              # type: ignore
                                               retriever=in__vectordb.as_retriever(),
                                               verbose=True,
                                               memory=memory,
                                               return_source_documents=True)


    result = qa({"question": in__question})

    # Create the JSON response object
    total_response = {
        "question": result["question"],
        "result": {
            "answer": result["answer"],
            "source_documents": result["source_documents"]
        }
    }

    return total_response



if __name__ == "__main__":
    llm_temp0 = OpenAI(temperature=0)       #type: ignore

    response = process_question_chained(in__question="How to describe VSM in three bullet points?",
                                        in__llm=llm_temp0,
                                        in__vectordb=vectordb)
    print(response)
